



```{r include=FALSE}
library(tidyverse)
library(e1071)  


house_data <- read_csv("../01_data/house_data.csv")
house_data$object_type_name <- as.factor(house_data$object_type_name)
house_data$num_rooms <- as.integer(house_data$num_rooms)
house_data <- house_data %>% select(object_type_name,
                                    build_year,
                                    living_area,
                                    zipcode,
                                    municipality_name,
                                    num_rooms,
                                    travel_time_private_transport,
                                    travel_time_public_transport,
                                    number_of_buildings_in_hectare,
                                    number_of_apartments_in_hectare,
                                    number_of_workplaces_in_hectare,
                                    population_in_hectare,
                                    water_percentage_1000,
                                    price)

# Log-Transform dependent variable
house_data$price.log <- log(house_data$price)
# Fix the predictors zipcode and num_rooms to be a factor
house_data$zipcode.fac <- factor(house_data$zipcode)
house_data$num_rooms.fac <- factor(house_data$num_rooms)


```

\pagebreak

## Support vector machines

The following chapter covers Support Vectore Machines. These are used to handle classification problems with a margin between separation points and their support vectors. 
In the following case, an attempt is made to classify the type of the residential object using price, number of rooms and  living area as predictors.

to improve the initial state of classification, only two classes are provided to the SVM. In addition, outliers in terms of the size of the living area and the number of rooms have been restricted.

```{r results = 'hide'}


house_data <- house_data %>% filter(object_type_name ==  "Einfamilienhaus"|object_type_name ==  "Wohnung")
house_data <- house_data %>% filter(living_area <= 1000)
house_data <- house_data %>% filter(num_rooms <= 20)

house_data <- house_data %>%  droplevels()
house_data$object_type_name %>%  unique()

## 75% of the sample size
smp_size <- floor(0.75 * nrow(house_data))

## set the seed to make your partition reproducible
set.seed(1)
train_ind <- sample(seq_len(nrow(house_data)), size = smp_size)
house_train <- house_data[train_ind, ]
house_test <- house_data[-train_ind, ]

```

In a first step, the different levels of determination are now considered. 

Logarithmic price and the living area


```{r echo=FALSE}
ggplot(house_data, aes(x=living_area, y=price.log)) + geom_point()
```


Logarithmic price and number of rooms

```{r echo=FALSE}
ggplot(house_data, aes(x=num_rooms, y=price.log)) + geom_point()
```




Now that the data for the problem is prepared, we try to apply the SVM to it.
in the following section three different kernels (linear,sigmoid,radial) are used, all of them having received the same date for classification.



## SVM with linear kernel


```{r results = FALSE , message=FALSE, warning=FALSE}
number_of_observations <- 200
cost_range <- c(0.000005, 0.005,0.1,1,10,100,1000,10000)
svm.models <- tune(
  svm,
  object_type_name ~ price.log + living_area + num_rooms,
  data = house_train %>% top_n(number_of_observations),
  kernel = "linear",
  ranges = list(cost = cost_range),
  scale = TRUE)

svm.models <- svm.models$best.model
summary(svm.models)


```
```{r echo=FALSE , message=FALSE, warning=FALSE}
plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ living_area)
```

```{r echo=FALSE , message=FALSE, warning=FALSE}

plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ num_rooms)

```

```{r}

table(predict = predict(svm.models, house_train),truth = house_train$object_type_name)
```



## SVM with sigmoid kernel


```{r results = FALSE , message=FALSE, warning=FALSE}
number_of_observations <- 10000
cost_range <- c(0.000005, 0.005,0.1,1,10,100,1000,10000)
svm.models <- tune(
  svm,
  object_type_name ~ price.log + living_area + num_rooms,
  data = house_train %>% top_n(number_of_observations),
  kernel = "sigmoid",
  ranges = list(cost = cost_range),
  scale = TRUE)
svm.models <- svm.models$best.model
summary(svm.models)


```


```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ living_area)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}

plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ num_rooms)

```

```{r}

table(predict = predict(svm.models, house_train),truth = house_train$object_type_name)
```

## SVM with radial kernel


```{r results = FALSE , message=FALSE, warning=FALSE}
number_of_observations <- 1000
cost_range <- c(0.000005, 0.005,0.1,1,10,100,1000,10000)
svm.models <- tune(
  svm,
  object_type_name ~ price.log + living_area + num_rooms,
  data = house_train %>% top_n(number_of_observations),
  kernel = "radial",
  ranges = list(cost = cost_range),
  scale = TRUE)

svm.models <- svm.models$best.model
summary(svm.models)



```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ living_area)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}

plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ num_rooms)

```

```{r}

table(predict = predict(svm.models, house_train),truth = house_train$object_type_name)
```





## Conclusion 

we see that the SVM using the sigmoid kernel has the best classification rate.
The other SVMs using the linear and radial kernel could not provide a sufficient separation layer

It is not possible to differentiate clearly between an apartment and a single-family house.
Therefore, the SVM is not suitable for classification in this case.
This can be clearly seen from the fact that the data points in the graphs clearly overlap. 
The SVM can therefore not make a proper separation of the two classes.



