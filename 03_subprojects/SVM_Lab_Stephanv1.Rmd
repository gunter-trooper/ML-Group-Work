



```{r include=FALSE}
library(tidyverse)
library(e1071)  


house_data <- read_csv("../01_data/house_data.csv")
house_data$object_type_name <- as.factor(house_data$object_type_name)
house_data <- house_data %>% select(object_type_name,
                                    build_year,
                                    living_area,
                                    zipcode,
                                    municipality_name,
                                    num_rooms,
                                    travel_time_private_transport,
                                    travel_time_public_transport,
                                    number_of_buildings_in_hectare,
                                    number_of_apartments_in_hectare,
                                    number_of_workplaces_in_hectare,
                                    population_in_hectare,
                                    water_percentage_1000,
                                    price)

# Log-Transform dependent variable
house_data$price.log <- log(house_data$price)
# Fix the predictors zipcode and num_rooms to be a factor
house_data$zipcode.fac <- factor(house_data$zipcode)
house_data$num_rooms.fac <- factor(house_data$num_rooms)


```

\pagebreak

## Support vector machines

The following chapter covers Support Vectore Machines. These are used to handle classification problems with a margin between separation points and their support vectors. 
In the following case, an attempt is made to classify the type of the residential object using price, number of rooms and  living area as predictors.

to improve the initial state of classification, only two classes are provided to the SVM. In addition, outliers in terms of the size of the living area and the number of rooms have been restricted.

```{r results = 'hide'}


house_data <- house_data %>% filter(object_type_name ==  "Einfamilienhaus"|object_type_name ==  "Wohnung")
house_data <- house_data %>% filter(living_area <= 1000)
house_data <- house_data %>% filter(num_rooms <= 20)

house_data <- house_data %>%  droplevels()
house_data$object_type_name %>%  unique()

## 75% of the sample size
smp_size <- floor(0.75 * nrow(house_data))

## set the seed to make your partition reproducible
set.seed(1)
train_ind <- sample(seq_len(nrow(house_data)), size = smp_size)
house_train <- house_data[train_ind, ]
house_test <- house_data[-train_ind, ]

```

## Graphical analysis of the separating variables

The following plots should show how well the fed-in variables separate a classification of the two classes apartment and single family house.

The first plot shows how the variable room numbers separate the two classes. 
In this plot you can see that an apartment has less rooms than a family house. This fact could be used for a separation.
Furthermore it is obvious that the number of rooms in the range up to 10 rooms cannot be used as a separation criterion.


```{r echo=FALSE}
ggplot(house_data, aes(y=num_rooms, x=object_type_name)) + geom_point()
```

The second plot shows how the variable living area separates the two classes. 
The plot shows that the apartments contain a much smaller living space. 
The variable should therefore to a certain degree support a clean separation.
However, it can also be seen that there is a certain overlap in the lower area of the living space. 
the influence of this fact for the separation can not yet be estimated.


```{r echo=FALSE}
ggplot(house_data, aes(y=living_area, x=object_type_name)) + geom_point()
```


The last plot shows how far the classes can be distinguished by the logarithmic price.
It can be seen that the price for both classes is distributed almost identically. 
From a graphical point of view, the price is therefore only conditionally suitable for a separation.


```{r echo=FALSE}
ggplot(house_data, aes(y=price.log, x=object_type_name)) + geom_point()
```

Now that the data for the problem is prepared, we try to apply the SVM to it.
in the following section three different kernels (linear,sigmoid,radial) are used, all of them having received the same date for classification.



## SVM with linear kernel

The following code block shows the configuration of SVM with linear kernel.


```{r results = FALSE , message=FALSE, warning=FALSE}
number_of_observations <- 200
cost_range <- c(0.000005, 0.005,0.1,1,10,100,1000,10000)
svm.models <- tune(
  svm,
  object_type_name ~ price.log + living_area + num_rooms,
  data = house_train %>% top_n(number_of_observations),
  kernel = "linear",
  ranges = list(cost = cost_range),
  scale = TRUE)

svm.models <- svm.models$best.model
summary(svm.models)


```
```{r echo=FALSE , message=FALSE, warning=FALSE}
plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ living_area)
```

```{r echo=FALSE , message=FALSE, warning=FALSE}

plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ num_rooms)

```

```{r}

table(predict = predict(svm.models, house_train),truth = house_train$object_type_name)
```



## SVM with sigmoid kernel


The following code block shows the configuration of SVM with sigmoid kernel.


```{r results = FALSE , message=FALSE, warning=FALSE}
number_of_observations <- 10000
cost_range <- c(0.000005, 0.005,0.1,1,10,100,1000,10000)
svm.models <- tune(
  svm,
  object_type_name ~ price.log + living_area + num_rooms,
  data = house_train %>% top_n(number_of_observations),
  kernel = "sigmoid",
  ranges = list(cost = cost_range),
  scale = TRUE)
svm.models <- svm.models$best.model
summary(svm.models)


```


```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ living_area)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}

plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ num_rooms)

```

```{r}

table(predict = predict(svm.models, house_train),truth = house_train$object_type_name)
```

## SVM with radial kernel

The following code block shows the configuration of SVM with radial kernel.

```{r results = FALSE , message=FALSE, warning=FALSE}
number_of_observations <- 1000
cost_range <- c(0.000005, 0.005,0.1,1,10,100,1000,10000)
svm.models <- tune(
  svm,
  object_type_name ~ price.log + living_area + num_rooms,
  data = house_train %>% top_n(number_of_observations),
  kernel = "radial",
  ranges = list(cost = cost_range),
  scale = TRUE)

svm.models <- svm.models$best.model
summary(svm.models)



```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ living_area)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}

plot(x = svm.models, data = house_train %>% top_n(number_of_observations), formula = price.log ~ num_rooms)

```

```{r}

table(predict = predict(svm.models, house_train),truth = house_train$object_type_name)
```



## Conclusion 

We see that the SVM using the sigmoid kernel has the best classification rate.
The other SVMs using the linear and radial kernel could not provide a sufficient separation layer

It is not possible to differentiate clearly between an apartment and a single-family house.
Therefore, the SVM is not suitable for classification in this case.
This can be clearly seen from the fact that the data points in the graphs clearly overlap. 
The SVM can therefore not make a proper separation of the two classes.



